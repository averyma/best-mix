{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/ssd001/home/ama/workspace/ama-at-vector/best-mix\n"
     ]
    }
   ],
   "source": [
    "cd '/h/ama/workspace/ama-at-vector/best-mix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from mixup import mixup_graph\n",
    "import time\n",
    "from utils_mixup import gradmix_v2, gradmix_v2_improved_v3\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "%matplotlib inline\n",
    "\n",
    "from collections import OrderedDict\n",
    "args={'dataset': 'cifar100', 'data_dir': '/h/ama/workspace/ama-at-vector/best-mix/data', 'root_dir': '/scratch/hdd001/home/ama/mixup/2022-07-09/345324666', 'labels_per_class': 500, 'valid_labels_per_class': 0, 'arch': 'resnext29_4_24', 'initial_channels': 64, 'epochs': 300, 'method': 'ours', 'train': 'ours', 'in_batch': False, 'mixup_alpha': 1.0, 'dropout': False, 'box': False, 'graph': False, 'neigh_size': 4, 'n_labels': 3, 'beta': 1.2, 'gamma': 0.5, 'eta': 0.2, 'transport': True, 't_eps': 0.8, 't_size': -1, 'adv_eps': 10.0, 'adv_p': 0.0, 'clean_lam': 0.0, 'mp': 8, 'batch_size': 100, 'learning_rate': 0.2, 'momentum': 0.9, 'decay': 0.0001, 'schedule': [100, 200], 'gammas': [0.1, 0.1], 'print_freq': 100, 'resume': '', 'start_epoch': 0, 'evaluate': False, 'ngpu': 1, 'workers': 2, 'seed': 2, 'add_name': '', 'log_off': False, 'job_id': 7898345, 'enable_wandb': True, 'wandb_EOT': True, 'wandb_project': 'ours-with-shift-cifar100-resnext-fixed-sampling-update-ratio', 'wandb_log_freq': 10, 'job_name': '345324666', 'blur_sigma': 100.0, 'kernel_size': 5, 'wandb_mode': 'online', 'grad_normalization': 'L1', 'eval_mode': False, 'new_implementation': True, 'with_shift': True, 'use_yp_argmax': False, 'bce_saliency': True, 'rand_pos': 0.1, 'update_ratio': 1.0, 'prob_mix': 1.0, 'mix_schedule': 'fixed', 'mix_scheduled_epoch': 50, 'upper_lambda': 0.7, 'mixup_alpha2': 0.0, 'use_cuda': True}\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# args.eval_mode = True\n",
    "# args.dataset='cifar10'\n",
    "# args.update_ratio=0.5\n",
    "args.num_classes=100\n",
    "# args.blur_sigma=2\n",
    "# args.rand_pos=0.1\n",
    "# args.lr=0.2\n",
    "log = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "''' Model '''\n",
    "import models\n",
    "# import torchvision.models as models\n",
    "from load_data import load_data_subset\n",
    "from collections import OrderedDict\n",
    "\n",
    "labels_per_class = 500\n",
    "\n",
    "batch_size = 100\n",
    "workers=2\n",
    "dataset='cifar100'\n",
    "data_dir='data'\n",
    "valid_labels_per_class=0\n",
    "mixup_alpha=0\n",
    "train_loader, valid_loader, _, test_loader, num_classes = load_data_subset(batch_size,workers,dataset,data_dir,labels_per_class=labels_per_class,valid_labels_per_class=valid_labels_per_class,mixup_alpha=mixup_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# Codes are borrowed from https://github.com/vikasverma1077/manifold_mixup/tree/master/supervised\n",
    "\n",
    "import os, sys, shutil, time, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append('..')\n",
    "if sys.version_info[0] < 3:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import _pickle as pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from load_data import load_data_subset\n",
    "from logger import plotting, copy_script_to_folder, AverageMeter, RecorderMeter, time_string, convert_secs2time\n",
    "import models\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import ipdb\n",
    "from utils_log import wandbLogger, saveCheckpoint\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils_wip import gradmix, reweighted_lam, gradmix_v2, gradmix_v2_improved, gradmix_v2_improved_v2\n",
    "from mixup import to_one_hot, get_lambda\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch, args, log, mp=None):\n",
    "    '''train given model and dataloader'''\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    mixing_avg = []\n",
    "    prob_mix = get_prob_mix(args.mix_schedule, args.prob_mix, epoch, args.mix_scheduled_epoch)\n",
    "\n",
    "    if args.method == 'ours' and args.blur_sigma != 100:\n",
    "        blurrer = transforms.GaussianBlur(kernel_size=(args.kernel_size, args.kernel_size),\n",
    "                                          sigma=(args.blur_sigma, args.blur_sigma))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = input.cuda()\n",
    "        target = target.long().cuda()\n",
    "\n",
    "        unary = None\n",
    "        noise = None\n",
    "        adv_mask1 = 0\n",
    "        adv_mask2 = 0\n",
    "\n",
    "        # train with clean images\n",
    "        # integrating our method :AVery\n",
    "        if args.train == 'ours':\n",
    "\n",
    "            batch_size = input.shape[0]\n",
    "            mix_size = int(batch_size*prob_mix)\n",
    "\n",
    "            # if mix_size is 0, we are simply doing standard training with no DA\n",
    "            if mix_size == 0:\n",
    "                input_var, target_var = Variable(input), Variable(target)\n",
    "                output, reweighted_target = model(input_var, target_var)\n",
    "\n",
    "                loss = bce_loss(softmax(output), reweighted_target)\n",
    "            else:\n",
    "                if mix_size == batch_size:\n",
    "                    # entire batch is DA\n",
    "                    input_2b_mixed = input\n",
    "                    target_2b_mixed = target\n",
    "                    input_std = None\n",
    "                    target_std = None\n",
    "                else:\n",
    "                    # some inputs are augmented, some are not\n",
    "                    input_std, input_2b_mixed = input[:(batch_size-mix_size)], input[(batch_size-mix_size):]\n",
    "                    target_std, target_2b_mixed = target[:(batch_size-mix_size)], target[(batch_size-mix_size):]\n",
    "\n",
    "                input_2b_mixed_var = Variable(input_2b_mixed, requires_grad=True)\n",
    "                target_2b_mixed_var = Variable(target_2b_mixed)\n",
    "\n",
    "                # calculate saliency\n",
    "                if args.eval_mode:\n",
    "                    model.eval()\n",
    "                    print('in eval mode')\n",
    "                else:\n",
    "                    model.train()\n",
    "                    print('in train mode')\n",
    "\n",
    "#                 if args.bce_saliency:\n",
    "                reweighted_target = to_one_hot(target_2b_mixed_var,args.num_classes)\n",
    "                output = model(input_2b_mixed_var)\n",
    "                loss_batch_mean = bce_loss(softmax(output), reweighted_target)\n",
    "\n",
    "                if args.update_ratio != 1.:\n",
    "                    print('loss_batch mean adjusted')\n",
    "                    loss_batch_mean *= (1-args.update_ratio)\n",
    "\n",
    "#                 else:\n",
    "#                     output = model(input_2b_mixed_var)\n",
    "#                     if args.use_yp_argmax:\n",
    "#                         loss_batch = criterion_batch(output, output.argmax(dim=1))\n",
    "#                     else:\n",
    "#                         loss_batch = criterion_batch(output, target_2b_mixed_var)\n",
    "\n",
    "#                     loss_batch_mean = torch.mean(loss_batch, dim=0)\n",
    "\n",
    "                loss_batch_mean.backward(retain_graph=True)\n",
    "#                 ipdb.set_trace()\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                g = input_2b_mixed_var.grad.data.abs().mean(dim=1, keepdim=True).detach()\n",
    "\n",
    "                # apply gaussian bluring to the gradients\n",
    "                if args.blur_sigma == 100:\n",
    "                    if args.dataset == 'tiny-imagenet-200':\n",
    "                        _blur_sigma = np.random.uniform(low=2.0, high=3.0)\n",
    "                    else:\n",
    "                        _blur_sigma = np.random.uniform(low=1.0, high=2.0)\n",
    "                    blurrer = transforms.GaussianBlur(kernel_size=(args.kernel_size, args.kernel_size),\n",
    "                                                      sigma=(_blur_sigma, _blur_sigma))\n",
    "                g_tilde = blurrer(g)\n",
    "\n",
    "                if args.mixup_alpha == 0.:\n",
    "                    sampled_alpha = 0.5\n",
    "                else:\n",
    "                    sampled_alpha = get_lambda(args.mixup_alpha)\n",
    "                sampled_alpha *= args.upper_lambda\n",
    "\n",
    "                mixed_x, mixed_y, mixed_lam = gradmix_v2_improved_v3(input_2b_mixed_var,\n",
    "                                                                     target_2b_mixed_var,\n",
    "                                                                     g_tilde,\n",
    "                                                                     alpha=sampled_alpha,\n",
    "                                                                     normalization=args.grad_normalization,\n",
    "                                                                     debug=False,\n",
    "                                                                     rand_pos=args.rand_pos)\n",
    "\n",
    "                if args.update_ratio == 1.:\n",
    "                    print('zero grad')\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                reweighted_target_mix = reweighted_lam(mixed_y, mixed_lam, args.num_classes)\n",
    "                output_mix = model(mixed_x)\n",
    "                loss = bce_loss(softmax(output_mix), reweighted_target_mix)\n",
    "                if args.update_ratio != 1.:\n",
    "                    print('loss  adjusted')\n",
    "                    loss *= args.update_ratio\n",
    "                    \n",
    "                print(loss_batch_mean.item(), loss.item())\n",
    "\n",
    "    ########\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        if args.train in ['mixup', 'mixup_hidden']:\n",
    "            if mix_size == 0:\n",
    "                prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "            elif mix_size == batch_size:\n",
    "                prec1, prec5 = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "            else:\n",
    "                prec1_mix, prec5_mix = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "                prec1_std, prec5_std = accuracy(output_std, target_std, topk=(1, 5))\n",
    "                prec1 = prec1_mix + prec1_std\n",
    "                prec5 = prec5_mix + prec5_std\n",
    "        elif args.train == 'ours':\n",
    "            if mix_size == 0:\n",
    "                prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "            elif mix_size == batch_size:\n",
    "                prec1, prec5 = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "            else:\n",
    "                if args.new_implementation:\n",
    "                    prec1, prec5 = accuracy(output_mix, target_concat, topk=(1, 5))\n",
    "                else:\n",
    "                    prec1_mix, prec5_mix = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "                    prec1_std, prec5_std = accuracy(output_std, target_std, topk=(1, 5))\n",
    "                    prec1 = prec1_mix + prec1_std\n",
    "                    prec5 = prec5_mix + prec5_std\n",
    "        elif args.train == 'vanilla':\n",
    "            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    if (epoch == 0 or (epoch+1)%args.wandb_log_freq == 0 or (epoch+1) == args.epochs) or args.wandb_EOT:\n",
    "        print_log(\n",
    "            '  **Train** Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Error@1 {error1:.3f}'.format(\n",
    "                top1=top1, top5=top5, error1=100 - top1.avg), log)\n",
    "    return top1.avg, top5.avg, losses.avg\n",
    "\n",
    "def get_prob_mix(mix_schedule, max_prob, epoch, scheduled_epoch):\n",
    "    \"\"\"\n",
    "    with scheldued mix,\n",
    "    if epoch < scheduled_epoch, we linearly increase the mix prob from 0 to max_prob\n",
    "    if epoch > scheduled_epoch, we fix the prob at max_prob\n",
    "    \"\"\"\n",
    "    if mix_schedule == 'fixed':\n",
    "        prob_mix = max_prob\n",
    "    elif mix_schedule == 'scheduled':\n",
    "        if epoch+1>=scheduled_epoch:\n",
    "            prob_mix = max_prob\n",
    "        else:\n",
    "            prob_mix = (epoch+1)/scheduled_epoch*max_prob\n",
    "    elif mix_schedule == 'delayed':\n",
    "        if epoch>=scheduled_epoch:\n",
    "            prob_mix = max_prob\n",
    "        else:\n",
    "            prob_mix = 0\n",
    "    return prob_mix\n",
    "\n",
    "bce_loss = nn.BCELoss().cuda()\n",
    "bce_loss_sum = nn.BCELoss(reduction='sum').cuda()\n",
    "softmax = nn.Softmax(dim=1).cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "criterion_batch = nn.CrossEntropyLoss(reduction='none').cuda()\n",
    "\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def print_log(print_string, log, end='\\n'):\n",
    "    '''print log'''\n",
    "    print(\"{}\".format(print_string), end=end)\n",
    "    if log is not None:\n",
    "        if end == '\\n':\n",
    "            log.write('{}\\n'.format(print_string))\n",
    "        else:\n",
    "            log.write('{} '.format(print_string))\n",
    "        log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.arch = 'preactresnet18'\n",
    "resnet = models.__dict__[args.arch](100, False, 1).cuda()\n",
    "\n",
    "# checkpoint = torch.load('checkpoint/cifar10_preact_ckpt_vanilla.pth.tar')\n",
    "\n",
    "# od = OrderedDict()\n",
    "# for key in checkpoint['state_dict'].keys():\n",
    "#     od[key[7:]] = checkpoint['state_dict'][key]\n",
    "# resnet.load_state_dict(od)\n",
    "\n",
    "# resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(resnet.parameters(),\n",
    "                                0.2,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-3,\n",
    "                                nesterov=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.027957884594798088 0.028081579133868217\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.028034869581460953 0.028320247307419777\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.028011035174131393 0.028156865388154984\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.027983684092760086 0.028139855712652206\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.02799263782799244 0.028008535504341125\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.02797263115644455 0.027946028858423233\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.028051042929291725 0.028378862887620926\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.028050456196069717 0.028182636946439743\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.028000609949231148 0.027864113450050354\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n",
      "loss  adjusted\n",
      "0.027964921668171883 0.028019560500979424\n",
      "in eval mode\n",
      "loss_batch mean adjusted\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31045/3435193209.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mtic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m#     try:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;31m#     except:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m#         ipdb.set_trace()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_31045/4003855528.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(train_loader, model, optimizer, epoch, args, log, mp)\u001b[0m\n\u001b[1;32m    136\u001b[0m                 \u001b[0msampled_alpha\u001b[0m \u001b[0;34m*=\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupper_lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                 mixed_x, mixed_y, mixed_lam = gradmix_v2_improved_v3(input_2b_mixed_var,\n\u001b[0m\u001b[1;32m    139\u001b[0m                                                                      \u001b[0mtarget_2b_mixed_var\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                                                                      \u001b[0mg_tilde\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/scratch/ssd001/home/ama/workspace/ama-at-vector/best-mix/utils_mixup.py\u001b[0m in \u001b[0;36mgradmix_v2_improved_v3\u001b[0;34m(x, y, grad, alpha, normalization, debug, rand_pos)\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mM\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpadded_normalized_grad_1\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpadded_normalized_grad_1\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mpadded_normalized_grad_2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1e-6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 315\u001b[0;31m         \u001b[0mcurrent_saliency\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_center\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_normalized_grad_1\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpadded_normalized_grad_2\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mM\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    316\u001b[0m         \u001b[0mcriteria\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcurrent_saliency\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0mupdate_needed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcriteria\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mmax_criteria\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "args.eval_mode = 1\n",
    "args.update_ratio = 0.5\n",
    "args.method = 'ours'\n",
    "args.train = 'ours'\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "#     try:\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "#     except:\n",
    "#         ipdb.set_trace()\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "new-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
