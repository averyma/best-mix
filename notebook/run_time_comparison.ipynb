{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/ssd001/home/ama/workspace/ama-at-vector/best-mix\n"
     ]
    }
   ],
   "source": [
    "cd '/h/ama/workspace/ama-at-vector/best-mix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "args = {'dataset': 'cifar10', 'data_dir': '/h/ama/workspace/ama-at-vector/best-mix/data', 'root_dir': '/scratch/hdd001/home/ama/mixup/2022-07-02/11', 'labels_per_class': 5000, 'valid_labels_per_class': 0, 'arch': 'preactresnet18', 'initial_channels': 64, 'epochs': 300, 'method': 'vanilla', 'train': 'vanilla', 'in_batch': False, 'mixup_alpha': 1.0, 'dropout': False, 'box': False, 'graph': False, 'neigh_size': 4, 'n_labels': 3, 'beta': 1.2, 'gamma': 0.5, 'eta': 0.2, 'transport': True, 't_eps': 0.8, 't_size': -1, 'adv_eps': 10.0, 'adv_p': 0.0, 'clean_lam': 0.0, 'mp': 8, 'batch_size': 100, 'learning_rate': 0.2, 'momentum': 0.9, 'decay': 0.0001, 'schedule': [100, 200], 'gammas': [0.1, 0.1], 'print_freq': 100, 'resume': '', 'start_epoch': 0, 'evaluate': False, 'ngpu': 1, 'workers': 2, 'seed': 1, 'add_name': '', 'log_off': False, 'job_id': 7889109, 'enable_wandb': False, 'wandb_project': 'test', 'job_name': '11', 'blur_sigma': 1.0, 'kernel_size': 5, 'grad_normalization': 'L1', 'eval_mode': False, 'new_implementation': True, 'with_shift': True, 'use_yp_argmax': False, 'mix_stride': 2, 'rand_pos': 1, 'prob_mix': 1.0, 'mix_schedule': 'fixed', 'mix_scheduled_epoch': 50, 'upper_lambda': 0.5, 'mixup_alpha2': 0.0, 'use_cuda': True}\n",
    "\n",
    "class dotdict(dict):\n",
    "    \"\"\"dot.notation access to dictionary attributes\"\"\"\n",
    "    __getattr__ = dict.get\n",
    "    __setattr__ = dict.__setitem__\n",
    "    __delattr__ = dict.__delitem__\n",
    "\n",
    "# mydict = {'val':'it works'}\n",
    "# nested_dict = {'val':'nested works too'}\n",
    "args = dotdict(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args.dataset='cifar10'\n",
    "args.num_classes=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "log = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from mixup import mixup_graph\n",
    "import time\n",
    "from utils_mixup import gradmix_v2, gradmix_v2_improved\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "''' Model '''\n",
    "import models\n",
    "# import torchvision.models as models\n",
    "from load_data import load_data_subset\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "resnet = models.__dict__['preactresnet18'](10, False, 1).cuda()\n",
    "\n",
    "# checkpoint = torch.load('/group-volume/Multimodal-Learning/ssl/vse_files/runs/fast_autoaugment/models/cifar10_preact_ckpt/vanilla.pth.tar')\n",
    "checkpoint = torch.load('checkpoint/cifar10_preact_ckpt_vanilla.pth.tar')\n",
    "\n",
    "od = OrderedDict()\n",
    "for key in checkpoint['state_dict'].keys():\n",
    "    od[key[7:]] = checkpoint['state_dict'][key]\n",
    "resnet.load_state_dict(od)\n",
    "\n",
    "# resnet = models.resnet18(pretrained=True)\n",
    "\n",
    "optimizer = torch.optim.SGD(resnet.parameters(),\n",
    "                                0.2,\n",
    "                                momentum=0.9,\n",
    "                                weight_decay=1e-3,\n",
    "                                nesterov=True)\n",
    "\n",
    "\n",
    "\n",
    "# resnet.load_state_dict(checkpoint['state_dict'])\n",
    "# mean = torch.tensor([x / 255 for x in [125.3, 123.0, 113.9]],dtype=torch.float32).reshape(1, 3, 1, 1).cuda()\n",
    "# std = torch.tensor([x / 255 for x in [63.0, 62.1, 66.7]], dtype=torch.float32).reshape(1, 3, 1, 1).cuda()\n",
    "labels_per_class = 5000\n",
    "# mean = torch.tensor([125.3, 123.0, 113.9])/255\n",
    "# std = torch.tensor([63.0, 62.1, 66.7])/255\n",
    "# mean_torch = mean.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "# std_torch = std.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "workers=2\n",
    "dataset='cifar10'\n",
    "# data_dir='/group-volume/Multimodal-Learning/ssl/vse_files/runs/fast_autoaugment/data'\n",
    "data_dir='data'\n",
    "valid_labels_per_class=0\n",
    "mixup_alpha=0\n",
    "train_loader, valid_loader, _, test_loader, num_classes = load_data_subset(batch_size,workers,dataset,data_dir,labels_per_class=labels_per_class,valid_labels_per_class=valid_labels_per_class,mixup_alpha=mixup_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "# Codes are borrowed from https://github.com/vikasverma1077/manifold_mixup/tree/master/supervised\n",
    "\n",
    "import os, sys, shutil, time, random\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append('..')\n",
    "if sys.version_info[0] < 3:\n",
    "    import cPickle as pickle\n",
    "else:\n",
    "    import _pickle as pickle\n",
    "import argparse\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from load_data import load_data_subset\n",
    "from logger import plotting, copy_script_to_folder, AverageMeter, RecorderMeter, time_string, convert_secs2time\n",
    "import models\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import ipdb\n",
    "from utils_log import wandbLogger, saveCheckpoint\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from utils_mixup import gradmix, reweighted_lam, gradmix_v2, gradmix_v2_improved\n",
    "from mixup import to_one_hot, get_lambda\n",
    "\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch, args, log, mp=None):\n",
    "    '''train given model and dataloader'''\n",
    "    batch_time = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "    top5 = AverageMeter()\n",
    "    mixing_avg = []\n",
    "    prob_mix = get_prob_mix(args.mix_schedule, args.prob_mix, epoch, args.mix_scheduled_epoch)\n",
    "\n",
    "    # if args.blur_sigma != 0.:\n",
    "    blurrer = transforms.GaussianBlur(kernel_size=(args.kernel_size, args.kernel_size),\n",
    "                                      sigma=(args.blur_sigma, args.blur_sigma))\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        data_time.update(time.time() - end)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input = input.cuda()\n",
    "        target = target.long().cuda()\n",
    "\n",
    "        unary = None\n",
    "        noise = None\n",
    "        adv_mask1 = 0\n",
    "        adv_mask2 = 0\n",
    "\n",
    "        # train with clean images\n",
    "        if args.train == 'vanilla':\n",
    "            input_var, target_var = Variable(input), Variable(target)\n",
    "            output, reweighted_target = model(input_var, target_var)\n",
    "            loss = bce_loss(softmax(output), reweighted_target)\n",
    "#             break\n",
    "\n",
    "        # train with mixup images\n",
    "        elif args.train == 'mixup':\n",
    "\n",
    "            batch_size = input.shape[0]\n",
    "            mix_size = int(batch_size*prob_mix)\n",
    "\n",
    "            # if mix_size is 0, we are simply doing standard training with no DA\n",
    "            if mix_size == 0:\n",
    "                input_var, target_var = Variable(input), Variable(target)\n",
    "                output, reweighted_target = model(input_var, target_var)\n",
    "\n",
    "                loss = bce_loss(softmax(output), reweighted_target)\n",
    "            else:\n",
    "                if mix_size == batch_size:\n",
    "                    # entire batch is DA\n",
    "                    input_2b_mixed = input\n",
    "                    target_2b_mixed = target\n",
    "                    input_std = None\n",
    "                    target_std = None\n",
    "                else:\n",
    "                    # some inputs are augmented, some are not\n",
    "                    input_std, input_2b_mixed = input[:(batch_size-mix_size)], input[(batch_size-mix_size):]\n",
    "                    target_std, target_2b_mixed = target[:(batch_size-mix_size)], target[(batch_size-mix_size):]\n",
    "\n",
    "                # process for Puzzle Mix\n",
    "                if args.graph:\n",
    "                    # whether to add adversarial noise or not\n",
    "                    if args.adv_p > 0:\n",
    "                        adv_mask1 = np.random.binomial(n=1, p=args.adv_p)\n",
    "                        adv_mask2 = np.random.binomial(n=1, p=args.adv_p)\n",
    "                    else:\n",
    "                        adv_mask1 = 0\n",
    "                        adv_mask2 = 0\n",
    "\n",
    "                    # random start:\n",
    "                    if (adv_mask1 == 1 or adv_mask2 == 1):\n",
    "                        noise = torch.zeros_like(input_2b_mixed).uniform_(-args.adv_eps / 255.,\n",
    "                                                                 args.adv_eps / 255.)\n",
    "                        input_2b_mixed_orig = input_2b_mixed * args.std + args.mean\n",
    "                        input_2b_mixed_noise = input_2b_mixed_orig + noise\n",
    "                        input_2b_mixed_noise = torch.clamp(input_2b_mixed_noise, 0, 1)\n",
    "                        noise = input_2b_mixed_noise - input_2b_mixed_orig\n",
    "                        input_2b_mixed_noise = (input_2b_mixed_noise - args.mean) / args.std\n",
    "                        input_2b_mixed_var = Variable(input_2b_mixed_noise, requires_grad=True)\n",
    "                    else:\n",
    "                        input_2b_mixed_var = Variable(input_2b_mixed, requires_grad=True)\n",
    "                    target_2b_mixed_var = Variable(target_2b_mixed)\n",
    "\n",
    "                    # calculate saliency (unary)\n",
    "                    if args.clean_lam == 0:\n",
    "                        model.eval()\n",
    "                        output_mix = model(input_2b_mixed_var)\n",
    "                        loss_batch = criterion_batch(output_mix, target_2b_mixed_var)\n",
    "                    else:\n",
    "                        model.train()\n",
    "                        output_mix = model(input_2b_mixed_var)\n",
    "                        loss_batch = 2 * args.clean_lam * criterion_batch(output_mix,\n",
    "                                                                          target_2b_mixed_var) / args.num_classes\n",
    "\n",
    "                    loss_batch_mean = torch.mean(loss_batch, dim=0)\n",
    "                    loss_batch_mean.backward(retain_graph=True)\n",
    "\n",
    "                    unary = torch.sqrt(torch.mean(input_2b_mixed_var.grad**2, dim=1))\n",
    "\n",
    "                    # calculate adversarial noise\n",
    "                    if (adv_mask1 == 1 or adv_mask2 == 1):\n",
    "                        noise += (args.adv_eps + 2) / 255. * input_2b_mixed_var.grad.sign()\n",
    "                        noise = torch.clamp(noise, -args.adv_eps / 255., args.adv_eps / 255.)\n",
    "                        adv_mix_coef = np.random.uniform(0, 1)\n",
    "                        noise = adv_mix_coef * noise\n",
    "\n",
    "                    if args.clean_lam == 0:\n",
    "                        model.train()\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                input_2b_mixed_var, target_2b_mixed_var = Variable(input_2b_mixed), Variable(target_2b_mixed)\n",
    "                output_mix, reweighted_target = model(input_2b_mixed_var,\n",
    "                                                      target_2b_mixed_var,\n",
    "                                                      mixup=True,\n",
    "                                                      args=args,\n",
    "                                                      grad=unary,\n",
    "                                                      noise=noise,\n",
    "                                                      adv_mask1=adv_mask1,\n",
    "                                                      adv_mask2=adv_mask2,\n",
    "                                                      mp=mp)\n",
    "                if input_std is None:\n",
    "                    # perform mixup and calculate loss\n",
    "                    loss = bce_loss(softmax(output_mix), reweighted_target)\n",
    "                else:\n",
    "                    loss_mix = bce_loss_sum(softmax(output_mix), reweighted_target)\n",
    "\n",
    "                    input_std_var, target_std_var = Variable(input_std), Variable(target_std)\n",
    "                    output_std, reweighted_target_std = model(input_std_var, target_std_var)\n",
    "\n",
    "                    loss_std = bce_loss_sum(softmax(output_std), reweighted_target_std)\n",
    "                    loss = (loss_std+loss_mix)/batch_size/args.num_classes\n",
    "#             break\n",
    "\n",
    "        # integrating our method :AVery\n",
    "        elif args.train == 'ours':\n",
    "\n",
    "            batch_size = input.shape[0]\n",
    "            mix_size = int(batch_size*prob_mix)\n",
    "\n",
    "            # if mix_size is 0, we are simply doing standard training with no DA\n",
    "            if mix_size == 0:\n",
    "                input_var, target_var = Variable(input), Variable(target)\n",
    "                output, reweighted_target = model(input_var, target_var)\n",
    "\n",
    "                loss = bce_loss(softmax(output), reweighted_target)\n",
    "            else:\n",
    "                if mix_size == batch_size:\n",
    "                    # entire batch is DA\n",
    "                    input_2b_mixed = input\n",
    "                    target_2b_mixed = target\n",
    "                    input_std = None\n",
    "                    target_std = None\n",
    "                else:\n",
    "                    # some inputs are augmented, some are not\n",
    "                    input_std, input_2b_mixed = input[:(batch_size-mix_size)], input[(batch_size-mix_size):]\n",
    "                    target_std, target_2b_mixed = target[:(batch_size-mix_size)], target[(batch_size-mix_size):]\n",
    "\n",
    "                input_2b_mixed_var = Variable(input_2b_mixed, requires_grad=True)\n",
    "                target_2b_mixed_var = Variable(target_2b_mixed)\n",
    "\n",
    "                # calculate saliency\n",
    "                if args.eval_mode:\n",
    "                    model.eval()\n",
    "                else:\n",
    "                    model.train()\n",
    "                output = model(input_2b_mixed_var)\n",
    "\n",
    "                if args.use_yp_argmax:\n",
    "                    loss_batch = criterion_batch(output, output.argmax(dim=1))\n",
    "                else:\n",
    "                    loss_batch = criterion_batch(output, target_2b_mixed_var)\n",
    "\n",
    "                loss_batch_mean = torch.mean(loss_batch, dim=0)\n",
    "                loss_batch_mean.backward(retain_graph=True)\n",
    "\n",
    "                model.train()\n",
    "\n",
    "                g = input_2b_mixed_var.grad.data.abs().mean(dim=1, keepdim=True).detach()\n",
    "\n",
    "                # apply gaussian bluring to the gradients\n",
    "#                 if args.blur_sigma == 100:\n",
    "#                     _blur_sigma = np.random.uniform(low=1.0, high=2.0)\n",
    "#                 else:\n",
    "#                     _blur_sigma = args.blur_sigma\n",
    "#                 blurrer = transforms.GaussianBlur(kernel_size=(args.kernel_size, args.kernel_size),\n",
    "#                                                   sigma=(_blur_sigma, _blur_sigma))\n",
    "                g_tilde = blurrer(g)\n",
    "\n",
    "                if args.with_shift:\n",
    "                    if args.mixup_alpha2 == 0.:\n",
    "                        if args.mixup_alpha == 0.:\n",
    "                            sampled_alpha = 0.5\n",
    "                        else:\n",
    "                            sampled_alpha = get_lambda(args.mixup_alpha)\n",
    "                        sampled_alpha *= args.upper_lambda\n",
    "                    else:\n",
    "                        sampled_alpha = get_lambda(args.mixup_alpha, args.mixup_alpha2)\n",
    "                    \n",
    "#                     if args.arch == 'resnext29_4_24':\n",
    "                    mixed_x, mixed_y, mixed_lam = gradmix_v2_improved(input_2b_mixed_var,\n",
    "                                                                     target_2b_mixed_var,\n",
    "                                                                     g_tilde,\n",
    "                                                                     alpha=sampled_alpha,\n",
    "                                                                     normalization=args.grad_normalization,\n",
    "                                                                     stride=args.mix_stride,\n",
    "                                                                     debug=False,\n",
    "                                                                     rand_pos=args.rand_pos)\n",
    "                else:\n",
    "                    mixed_x, mixed_y, mixed_lam = gradmix(input_2b_mixed_var,\n",
    "                                                          target_2b_mixed_var,\n",
    "                                                          g_tilde)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                if args.new_implementation:\n",
    "                    reweighted_target_mix = reweighted_lam(mixed_y, mixed_lam, args.num_classes)\n",
    "\n",
    "                    if input_std is None:\n",
    "                        output_mix = model(mixed_x)\n",
    "                        loss = bce_loss(softmax(output_mix), reweighted_target_mix)\n",
    "                    else:\n",
    "                        input_std_var, target_std_var = Variable(input_std), Variable(target_std)\n",
    "                        input_concat = torch.cat([mixed_x, input_std_var], dim=0)\n",
    "\n",
    "                        output_mix = model(input_concat)\n",
    "\n",
    "                        reweighted_target_std = to_one_hot(target_std_var, args.num_classes)\n",
    "                        reweighted_target_concat = torch.cat([reweighted_target_mix, reweighted_target_std], dim=0)\n",
    "\n",
    "                        target_concat = torch.cat([target_2b_mixed, target_std], dim=0)\n",
    "\n",
    "                        loss = bce_loss(softmax(output_mix), reweighted_target_concat)\n",
    "                else:\n",
    "                    # perform mixup and calculate loss\n",
    "                    reweighted_target = reweighted_lam(mixed_y, mixed_lam, args.num_classes)\n",
    "                    output_mix = model(mixed_x)\n",
    "                    if input_std is None:\n",
    "                        loss = bce_loss(softmax(output_mix), reweighted_target)\n",
    "                    else:\n",
    "                        loss_mix = bce_loss_sum(softmax(output_mix), reweighted_target)\n",
    "\n",
    "                        input_std_var, target_std_var = Variable(input_std), Variable(target_std)\n",
    "                        output_std, reweighted_target_std = model(input_std_var, target_std_var)\n",
    "\n",
    "                        loss_std = bce_loss_sum(softmax(output_std), reweighted_target_std)\n",
    "\n",
    "                        loss = (loss_std+loss_mix)/batch_size/args.num_classes\n",
    "\n",
    "    ########\n",
    "\n",
    "        # for manifold mixup\n",
    "        elif args.train == 'mixup_hidden':\n",
    "            batch_size = input.shape[0]\n",
    "            mix_size = int(batch_size*prob_mix)\n",
    "\n",
    "            # if mix_size is 0, we are simply doing standard training with no DA\n",
    "            if mix_size == 0:\n",
    "                input_var, target_var = Variable(input), Variable(target)\n",
    "                if args.arch == 'resnext29_4_24':\n",
    "                    output = model(input_var)\n",
    "                    reweighted_target = to_one_hot(target_var, args.num_classes)\n",
    "                else:\n",
    "                    output, reweighted_target = model(input_var, target_var)\n",
    "\n",
    "                loss = bce_loss(softmax(output), reweighted_target)\n",
    "            else:\n",
    "                if mix_size == batch_size:\n",
    "                    # entire batch is DA\n",
    "                    input_2b_mixed = input\n",
    "                    target_2b_mixed = target\n",
    "                    input_std = None\n",
    "                    target_std = None\n",
    "                else:\n",
    "                    # some inputs are augmented, some are not\n",
    "                    input_std, input_2b_mixed = input[:(batch_size-mix_size)], input[(batch_size-mix_size):]\n",
    "                    target_std, target_2b_mixed = target[:(batch_size-mix_size)], target[(batch_size-mix_size):]\n",
    "\n",
    "                input_2b_mixed_var, target_2b_mixed_var = Variable(input_2b_mixed), Variable(target_2b_mixed)\n",
    "                output_mix, reweighted_target = model(input_2b_mixed_var, target_2b_mixed_var, mixup_hidden=True, args=args)\n",
    "\n",
    "                if input_std is None:\n",
    "                    # perform mixup and calculate loss\n",
    "                    loss = bce_loss(softmax(output_mix), reweighted_target)\n",
    "                else:\n",
    "                    loss_mix = bce_loss_sum(softmax(output_mix), reweighted_target)\n",
    "\n",
    "                    input_std_var, target_std_var = Variable(input_std), Variable(target_std)\n",
    "                    if args.arch == 'resnext29_4_24':\n",
    "                        output_std = model(input_std_var)\n",
    "                        reweighted_target_std = to_one_hot(target_std_var, args.num_classes)\n",
    "                    else:\n",
    "                        output_std, reweighted_target_std = model(input_std_var, target_std_var)\n",
    "\n",
    "                    loss_std = bce_loss_sum(softmax(output_std), reweighted_target_std)\n",
    "                    loss = (loss_std+loss_mix)/batch_size/args.num_classes\n",
    "#             break\n",
    "        else:\n",
    "            raise AssertionError('wrong train type!!')\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        if args.train in ['mixup', 'mixup_hidden']:\n",
    "            if mix_size == 0:\n",
    "                prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "            elif mix_size == batch_size:\n",
    "                prec1, prec5 = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "            else:\n",
    "                prec1_mix, prec5_mix = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "                prec1_std, prec5_std = accuracy(output_std, target_std, topk=(1, 5))\n",
    "                prec1 = prec1_mix + prec1_std\n",
    "                prec5 = prec5_mix + prec5_std\n",
    "        elif args.train == 'ours':\n",
    "            if mix_size == 0:\n",
    "                prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "            elif mix_size == batch_size:\n",
    "                prec1, prec5 = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "            else:\n",
    "                if args.new_implementation:\n",
    "                    prec1, prec5 = accuracy(output_mix, target_concat, topk=(1, 5))\n",
    "                else:\n",
    "                    prec1_mix, prec5_mix = accuracy(output_mix, target_2b_mixed, topk=(1, 5))\n",
    "                    prec1_std, prec5_std = accuracy(output_std, target_std, topk=(1, 5))\n",
    "                    prec1 = prec1_mix + prec1_std\n",
    "                    prec5 = prec5_mix + prec5_std\n",
    "        elif args.train == 'vanilla':\n",
    "            prec1, prec5 = accuracy(output, target, topk=(1, 5))\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "        top5.update(prec5.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "    print_log(\n",
    "        '  **Train** Prec@1 {top1.avg:.3f} Prec@5 {top5.avg:.3f} Error@1 {error1:.3f}'.format(\n",
    "            top1=top1, top5=top5, error1=100 - top1.avg), log)\n",
    "    return top1.avg, top5.avg, losses.avg\n",
    "\n",
    "def get_prob_mix(mix_schedule, max_prob, epoch, scheduled_epoch):\n",
    "    \"\"\"\n",
    "    with scheldued mix,\n",
    "    if epoch < scheduled_epoch, we linearly increase the mix prob from 0 to max_prob\n",
    "    if epoch > scheduled_epoch, we fix the prob at max_prob\n",
    "    \"\"\"\n",
    "    if mix_schedule == 'fixed':\n",
    "        prob_mix = max_prob\n",
    "    elif mix_schedule == 'scheduled':\n",
    "        if epoch+1>=scheduled_epoch:\n",
    "            prob_mix = max_prob\n",
    "        else:\n",
    "            prob_mix = (epoch+1)/scheduled_epoch*max_prob\n",
    "    elif mix_schedule == 'delayed':\n",
    "        if epoch>=scheduled_epoch:\n",
    "            prob_mix = max_prob\n",
    "        else:\n",
    "            prob_mix = 0\n",
    "    return prob_mix\n",
    "\n",
    "bce_loss = nn.BCELoss().cuda()\n",
    "bce_loss_sum = nn.BCELoss(reduction='sum').cuda()\n",
    "softmax = nn.Softmax(dim=1).cuda()\n",
    "criterion = nn.CrossEntropyLoss().cuda()\n",
    "criterion_batch = nn.CrossEntropyLoss(reduction='none').cuda()\n",
    "\n",
    "def accuracy(output, target, topk=(1, )):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.reshape(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "def print_log(print_string, log, end='\\n'):\n",
    "    '''print log'''\n",
    "    print(\"{}\".format(print_string), end=end)\n",
    "    if log is not None:\n",
    "        if end == '\\n':\n",
    "            log.write('{}\\n'.format(print_string))\n",
    "        else:\n",
    "            log.write('{} '.format(print_string))\n",
    "        log.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def change_args(args):\n",
    "    if args.method == 'vanilla':\n",
    "        args.train = 'vanilla'\n",
    "    elif args.method == 'input':\n",
    "        args.train = 'mixup'\n",
    "        if args.dataset in ['cifar10', 'cifar100']:\n",
    "            args.mixup_alpha = 1.0\n",
    "        else:\n",
    "            args.mixup_alpha = 0.2\n",
    "    elif args.method == 'manifold':\n",
    "        args.train = 'mixup_hidden'\n",
    "        if args.dataset in ['cifar10', 'cifar100']:\n",
    "            args.mixup_alpha = 2.0\n",
    "        else:\n",
    "            args.mixup_alpha = 0.2\n",
    "    elif args.method == 'cutmix':\n",
    "        args.train = 'mixup'\n",
    "        args.box = True\n",
    "        if args.dataset in ['cifar10', 'cifar100']:\n",
    "            args.mixup_alpha = 1.0\n",
    "        else:\n",
    "            args.mixup_alpha = 0.2\n",
    "    elif args.method == 'puzzle':\n",
    "        args.train = 'mixup'\n",
    "        args.graph = True\n",
    "        args.mixup_alpha = 1.0\n",
    "        args.n_labels = 3\n",
    "        args.eta = 0.2\n",
    "        args.beta = 1.2\n",
    "        args.gamma = 0.5\n",
    "        args.neigh_size = 4\n",
    "        args.transport = True\n",
    "        if args.dataset in ['cifar10', 'cifar100']:\n",
    "            args.t_size = 4\n",
    "        args.t_eps = 0.8\n",
    "        if args.dataset == 'tiny-imagenet-200':\n",
    "            args.clean_lam = 1\n",
    "    elif args.method == 'ours':\n",
    "        args.train = 'ours'\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model, Data, Saliency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vanilla\n",
      "  **Train** Prec@1 73.414 Prec@5 97.780 Error@1 26.586\n",
      "Average total time: 30.8060 seconds (var: 0.0000)\n",
      "Max/Min time: 30.8060/30.8060 seconds\n"
     ]
    }
   ],
   "source": [
    "args.method = 'vanilla'\n",
    "args = change_args(args)\n",
    "print(args.train)\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixup\n",
      "  **Train** Prec@1 44.088 Prec@5 80.734 Error@1 55.912\n",
      "Average total time: 31.0123 seconds (var: 0.0000)\n",
      "Max/Min time: 31.0123/31.0123 seconds\n"
     ]
    }
   ],
   "source": [
    "args.method = 'input'\n",
    "args = change_args(args)\n",
    "print(args.train)\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixup\n",
      "  **Train** Prec@1 48.766 Prec@5 88.740 Error@1 51.234\n",
      "Average total time: 31.1112 seconds (var: 0.0000)\n",
      "Max/Min time: 31.1112/31.1112 seconds\n"
     ]
    }
   ],
   "source": [
    "args.method = 'cutmix'\n",
    "args = change_args(args)\n",
    "print(args.train)\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixup_hidden\n",
      "  **Train** Prec@1 43.182 Prec@5 84.238 Error@1 56.818\n",
      "Average total time: 31.4363 seconds (var: 0.0000)\n",
      "Max/Min time: 31.4363/31.4363 seconds\n"
     ]
    }
   ],
   "source": [
    "args.method = 'manifold'\n",
    "args = change_args(args)\n",
    "print(args.train)\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mixup\n",
      "  **Train** Prec@1 49.922 Prec@5 89.536 Error@1 50.078\n",
      "Average total time: 59.6250 seconds (var: 0.0000)\n",
      "Max/Min time: 59.6250/59.6250 seconds\n"
     ]
    }
   ],
   "source": [
    "args.method = 'puzzle'\n",
    "args = change_args(args)\n",
    "print(args.train)\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ours\n",
      "  **Train** Prec@1 22.682 Prec@5 69.154 Error@1 77.318\n",
      "Average total time: 103.7237 seconds (var: 0.0000)\n",
      "Max/Min time: 103.7237/103.7237 seconds\n"
     ]
    }
   ],
   "source": [
    "args.method = 'ours'\n",
    "args = change_args(args)\n",
    "print(args.train)\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ours\n",
      "  **Train** Prec@1 22.566 Prec@5 69.608 Error@1 77.434\n",
      "Average total time: 104.4569 seconds (var: 0.0000)\n",
      "Max/Min time: 104.4569/104.4569 seconds\n"
     ]
    }
   ],
   "source": [
    "args.method = 'ours'\n",
    "args = change_args(args)\n",
    "print(args.train)\n",
    "\n",
    "total_time_list = []\n",
    "for i in range(1):\n",
    "    tic = time.perf_counter()\n",
    "    train(train_loader, resnet, optimizer, 0, args, log, mp=None)\n",
    "    toc = time.perf_counter()\n",
    "    total_time_list.append(toc-tic)\n",
    "\n",
    "total_time_list= np.array(total_time_list)\n",
    "print(f\"Average total time: {total_time_list.mean():0.4f} seconds (var: {total_time_list.var():0.4f})\")\n",
    "print(f\"Max/Min time: {total_time_list.max():0.4f}/{total_time_list.min():0.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "new-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
