{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/scratch/ssd001/home/ama/workspace/ama-at-vector/best-mix\n"
     ]
    }
   ],
   "source": [
    "cd '/h/ama/workspace/ama-at-vector/best-mix'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pickle\n",
    "\n",
    "from mixup import mixup_graph\n",
    "import time\n",
    "from utils_mixup import gradmix_v2, gradmix_v2_improved\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from load_data import load_data_subset\n",
    "\n",
    "labels_per_class = 5000\n",
    "batch_size = 100\n",
    "workers=2\n",
    "dataset='cifar10'\n",
    "# data_dir='/group-volume/Multimodal-Learning/ssl/vse_files/runs/fast_autoaugment/data'\n",
    "data_dir='data'\n",
    "valid_labels_per_class=0\n",
    "mixup_alpha=0\n",
    "train_loader, valid_loader, _, test_loader, num_classes = load_data_subset(batch_size,workers,dataset,data_dir,labels_per_class=labels_per_class,valid_labels_per_class=valid_labels_per_class,mixup_alpha=mixup_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Model '''\n",
    "import models\n",
    "# import torchvision.models as models\n",
    "from collections import OrderedDict\n",
    "\n",
    "\n",
    "\n",
    "model = models.__dict__['preactresnet18'](10, False, 1).cuda()\n",
    "\n",
    "ckpt_list = []\n",
    "# vanilla\n",
    "checkpoint_path = '/scratch/hdd001/home/ama/mixup/2022-06-23/14723466/cifar10_arch_preactresnet18_train_vanilla_eph_300_lr_0.2_transport_eps_0.8_size_-1_job_id_7853381_seed_0'\n",
    "checkpoint_path += '/checkpoint.pth.tar'\n",
    "ckpt_list.append(checkpoint_path)\n",
    "# #input\n",
    "checkpoint_path = '/scratch/hdd001/home/ama/mixup/2022-06-23/2982229464/cifar10_arch_preactresnet18_train_mixup_eph_300_lr_0.2_m_alpha_1.0_transport_eps_0.8_size_-1_job_id_7853392_seed_1'\n",
    "checkpoint_path += '/checkpoint.pth.tar'\n",
    "ckpt_list.append(checkpoint_path)\n",
    "# #cutmix\n",
    "checkpoint_path = '/scratch/hdd001/home/ama/mixup/2022-06-23/192377623/cifar10_arch_preactresnet18_train_mixup_eph_300_lr_0.2_m_alpha_1.0_box_transport_eps_0.8_size_-1_job_id_7853387_seed_0'\n",
    "checkpoint_path += '/checkpoint.pth.tar'\n",
    "ckpt_list.append(checkpoint_path)\n",
    "# #manifold\n",
    "checkpoint_path = '/scratch/hdd001/home/ama/mixup/2022-06-23/251261079/cifar10_arch_preactresnet18_train_mixup_hidden_eph_300_lr_0.2_m_alpha_2.0_transport_eps_0.8_size_-1_job_id_7853394_seed_1'\n",
    "checkpoint_path += '/checkpoint.pth.tar'\n",
    "ckpt_list.append(checkpoint_path)\n",
    "# #puzzle\n",
    "checkpoint_path = '/scratch/hdd001/home/ama/mixup/2022-06-23/114834856/cifar10_arch_preactresnet18_train_mixup_eph_300_lr_0.2_m_alpha_1.0_graph_n_labels_3_beta_1.2_gamma_0.5_neigh_4_eta_0.2_transport_eps_0.8_size_4_job_id_7853389_seed_0'\n",
    "checkpoint_path += '/checkpoint.pth.tar'\n",
    "ckpt_list.append(checkpoint_path)\n",
    "# #comix\n",
    "checkpoint_path = '/scratch/hdd001/home/ama/mixup/2022-06-23/31093/cifar10_per_5000_arch_preactresnet18_eph_300_lr_0.12_mblock_4_mbeta_0.32_mgamma_1.0_mthres_hard0.83_meta_0.05_m_alpha_2.0_mpart_20_niter_4_omega_0.001_set_clean_1.0_seed_2'\n",
    "checkpoint_path += '/checkpoint.pth.tar'\n",
    "ckpt_list.append(checkpoint_path)\n",
    "#ours\n",
    "checkpoint_path = '/scratch/hdd001/home/ama/mixup/2022-06-30/2361913365/cifar10_arch_preactresnet18_train_ours_eph_300_lr_0.2_m_alpha_1.0_transport_eps_0.8_size_-1_job_id_7886753_seed_1'\n",
    "checkpoint_path += '/checkpoint.pth.tar'\n",
    "ckpt_list.append(checkpoint_path)\n",
    "\n",
    "\n",
    "\n",
    "# mean = torch.tensor([x / 255 for x in [125.3, 123.0, 113.9]],dtype=torch.float32).reshape(1, 3, 1, 1).cuda()\n",
    "# std = torch.tensor([x / 255 for x in [63.0, 62.1, 66.7]], dtype=torch.float32).reshape(1, 3, 1, 1).cuda()\n",
    "# mean = torch.tensor([125.3, 123.0, 113.9])/255\n",
    "# std = torch.tensor([63.0, 62.1, 66.7])/255\n",
    "# mean_torch = mean.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "# std_torch = std.unsqueeze(0).unsqueeze(-1).unsqueeze(-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_clean(loader, model, device):\n",
    "    total_loss, total_correct = 0., 0.\n",
    "    for x,y in loader:\n",
    "        model.eval()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x)\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_hat, y)\n",
    "            batch_correct = (y_hat.argmax(dim = 1) == y).sum().item()\n",
    "        \n",
    "        total_correct += batch_correct\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        \n",
    "    test_acc = total_correct / len(loader.dataset) * 100\n",
    "    test_loss = total_loss / len(loader.dataset)\n",
    "    return test_acc, test_loss\n",
    "\n",
    "def test_clean(loader, model, device):\n",
    "    total_loss, total_correct = 0., 0.\n",
    "    for x,y in loader:\n",
    "        model.eval()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x)\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_hat, y)\n",
    "            batch_correct = (y_hat.argmax(dim = 1) == y).sum().item()\n",
    "        \n",
    "        total_correct += batch_correct\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        \n",
    "    test_acc = total_correct / len(loader.dataset) * 100\n",
    "    test_loss = total_loss / len(loader.dataset)\n",
    "    return test_acc, test_loss\n",
    "\n",
    "def test_fgsm_linf(loader, model, device):\n",
    "    total_loss, total_correct = 0., 0.\n",
    "    for x,y in loader:\n",
    "        model.eval()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        delta = torch.zeros_like(x, requires_grad = True).to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "        y_hat = model(x+delta)\n",
    "        loss = torch.nn.CrossEntropyLoss()(y_hat, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        delta.data = 8./255 *(62/255) * delta.grad.sign()\n",
    "        y_hat = model(x+delta.detach())\n",
    "        loss = torch.nn.CrossEntropyLoss()(y_hat, y)\n",
    "        \n",
    "        batch_correct = (y_hat.argmax(dim = 1) == y).sum().item()\n",
    "        \n",
    "        total_correct += batch_correct\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        \n",
    "    test_acc = total_correct / len(loader.dataset) * 100\n",
    "    test_loss = total_loss / len(loader.dataset)\n",
    "    return test_acc, test_loss\n",
    "\n",
    "def test_fgsm_l2(loader, model, device):\n",
    "    total_loss, total_correct = 0., 0.\n",
    "    for x,y in loader:\n",
    "        model.eval()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        delta = torch.zeros_like(x, requires_grad = True).to(device)\n",
    "\n",
    "#         with torch.no_grad():\n",
    "        y_hat = model(x+delta)\n",
    "        loss = torch.nn.CrossEntropyLoss()(y_hat, y)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        delta.data = 0.5 *(62/255) * delta.grad/torch.norm(delta.grad.detach(),p=2,dim=[1,2,3],keepdim=True)\n",
    "        y_hat = model(x+delta.detach())\n",
    "        loss = torch.nn.CrossEntropyLoss()(y_hat, y)\n",
    "        \n",
    "        batch_correct = (y_hat.argmax(dim = 1) == y).sum().item()\n",
    "        \n",
    "        total_correct += batch_correct\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        \n",
    "    test_acc = total_correct / len(loader.dataset) * 100\n",
    "    test_loss = total_loss / len(loader.dataset)\n",
    "    return test_acc, test_loss\n",
    "\n",
    "\n",
    "def test_gaussian(loader, model, var, device):\n",
    "    total_loss, total_correct = 0., 0.\n",
    "    for x,y in loader:\n",
    "        model.eval()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        \n",
    "        noise = (var**0.5)*torch.randn_like(x, device = x.device)\n",
    "#         ipdb.set_trace()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            y_hat = model(x+noise)\n",
    "            loss = torch.nn.CrossEntropyLoss()(y_hat, y)\n",
    "            batch_correct = (y_hat.argmax(dim = 1) == y).sum().item()\n",
    "        \n",
    "        total_correct += batch_correct\n",
    "        total_loss += loss.item() * x.shape[0]\n",
    "        \n",
    "    test_acc = total_correct / len(loader.dataset) * 100\n",
    "    test_loss = total_loss / len(loader.dataset)\n",
    "    return test_acc, test_loss\n",
    "\n",
    "CORRUPTIONS_CIFAR10=['brightness',         \n",
    "'gaussian_noise',    \n",
    "'saturate',\n",
    "'contrast',           \n",
    "'glass_blur',        \n",
    "'shot_noise',\n",
    "'defocus_blur',       \n",
    "'impulse_noise',     \n",
    "'snow',\n",
    "'elastic_transform',  \n",
    "'jpeg_compression',  \n",
    "'spatter',\n",
    "'fog',         \n",
    "'speckle_noise',\n",
    "'frost',              \n",
    "'motion_blur',       \n",
    "'zoom_blur',\n",
    "'gaussian_blur',      \n",
    "'pixelate'] \n",
    "\n",
    "def eval_corrupt(model, severity, device):\n",
    "#     dct_matrix = getDCTmatrix(28)\n",
    "    acc = []\n",
    "    total_correct = 0\n",
    "    \n",
    "    corruptions_list = CORRUPTIONS_CIFAR10\n",
    "    \n",
    "    for corrupt_type in corruptions_list:\n",
    "        data_path = '/h/ama/workspace/ama-at-vector/freq-robust/data/CIFAR-10-C/' + corrupt_type + '.npy'\n",
    "        label_path = '/h/ama/workspace/ama-at-vector/freq-robust/data/CIFAR-10-C/labels.npy'\n",
    "        x = torch.tensor(np.transpose(np.load(data_path), (0, 3, 1, 2))/255.,dtype= torch.float32)[(severity-1)*10000:severity*10000].to(device)\n",
    "        y = torch.tensor(np.load(label_path),dtype=torch.float32)[(severity-1)*10000:severity*10000].to(device)\n",
    "\n",
    "        for i in range(10):\n",
    "            with torch.no_grad():\n",
    "                y_hat = model(x[1000*i:1000*(i+1)])\n",
    "                total_correct += (y_hat.argmax(dim = 1) == y[1000*i:1000*(i+1)]).sum().item()\n",
    "        \n",
    "        corrupt_acc = total_correct / len(y) * 100\n",
    "        acc.append(corrupt_acc)\n",
    "        total_correct = 0\n",
    "        \n",
    "        del x,y \n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.58263157894737\n",
      "85.92526315789476\n",
      "77.15368421052632\n",
      "83.42263157894736\n",
      "79.19842105263159\n",
      "79.19526315789474\n",
      "80.56736842105263\n"
     ]
    }
   ],
   "source": [
    "for ckpt in ckpt_list:\n",
    "    checkpoint = torch.load(ckpt)\n",
    "\n",
    "\n",
    "    od = OrderedDict()\n",
    "    for key in checkpoint['state_dict'].keys():\n",
    "        od[key[7:]] = checkpoint['state_dict'][key]\n",
    "    model.load_state_dict(od)\n",
    "    \n",
    "#     print(test_clean(test_loader, model, 'cuda:0'))\n",
    "#     print(test_fgsm_linf(test_loader, model, 'cuda:0'))\n",
    "#     print(test_fgsm_l2(test_loader, model, 'cuda:0'))\n",
    "#     print(test_gaussian(test_loader, model, 0.1, 'cuda:0'))\n",
    "#     print(test_gaussian(test_loader, model, 0.1, 'cuda:0'))\n",
    "#     print(test_gaussian(test_loader, model, 0.5, 'cuda:0'))\n",
    "    print(np.array(eval_corrupt(model, 3, 'cuda:0')).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-env",
   "language": "python",
   "name": "new-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
